---
title: "Evaluation"
author: "Mohamed Bassem"
date: "April 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

```{r, results=FALSE,include=FALSE}
library("dplyr")
library("caret")
library("knitr")
```


## Helper functions

The function responsible for doing the k-fold cross validation, used in the caret::train function.

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           savePredictions = TRUE)
```

The evaluation performance function that calculates the accuracy, precision, etc ..

```{r}
evaluate_performance <- function(predicted, actual) {
  cmat <- confusionMatrix(predicted, actual)$table
  
  TP <- cmat[1,1]
  FN <- cmat[1,2]
  FP <- cmat[2,1]
  TN <- cmat[2,2]
  
  ret <- list(
    Accuracy = (TP+TN)/(TP+FN+FP+TN),
    Precision = (TP/(TP+FP)),
    Recall = (TP/(TP+FN))
  )
  
  ret$F1 <- (2*ret$Precision*ret$Recall)/(ret$Precision + ret$Recall)
  return(ret)
}

```

A function to split the dataset into obs and class required for training

```{r}
sample_data <- function(data, class_column_last) {
  data <- data[, colSums(is.na(data)) == 0 ]
  class_column <- 1
  if(class_column_last){
    class_column <- length(names(data))
  }
  
  if(is.integer(data[,class_column])){
    data[,class_column] <- as.factor(data[,class_column])
  }
  
  trainIdx <- createDataPartition(data[,class_column], p=.8, list=FALSE)
  train <- data[trainIdx, ]
  test <- data[-trainIdx, ]
  return(list(
    obs = data[,-class_column],
    class = data[,class_column]
  ))
}

```

```{r}
train_and_evaluate_model <- function(data, method){
  if(method == "nnet"){
    model <- train(data$obs, data$class, method = method, trControl=fitControl, trace=FALSE)
  }else{
    model <- train(data$obs, data$class, method = method, trControl=fitControl)
  }
  return(evaluate_performance(model$pred$pred, model$pred$obs))
}

```
```{r}
train_and_evaluate_all_models <- function(data){
  return(list(
    c45 = train_and_evaluate_model(data, "J48"),
    rf = train_and_evaluate_model(data, "rf"),
    svm = train_and_evaluate_model(data, "svmLinear"),
    nb = train_and_evaluate_model(data, "nb"),
    nnet = train_and_evaluate_model(data, "nnet"),
    bagging = train_and_evaluate_model(data, "treebag"),
    boosting = train_and_evaluate_model(data, "adaboost")
  ))
}
```

```{r}
datasets <- c("sonar_eval", "spect_eval", "pima_eval")
models <- c("c45", "rf", "svm", "nb", "nnet","bagging", "boosting")

format_comparison <- function(metric){
  ret <- list()
  
  for(dataset in datasets){
    ret[[dataset]] <- list()
    for(model in models){
      ret[[dataset]][[model]] <- get(dataset)[[model]][[metric]]
    }
  }
  return(ret)
}
```

## Loading Data

```{r}
sonar_data <- read.csv("./sonar/sonar.csv", header=FALSE) %>% sample_data(TRUE)
hepatitis_data <- read.csv("hepatitis/hepatitis.csv", header=FALSE) %>% dplyr::select(-V16, -V17) %>% sample_data(FALSE)
spect_data <- read.csv("SPECT/SPECT.train.csv", header = FALSE) %>% sample_data(FALSE)
pima_data <- read.csv("pima/pima-indians-diabetes.csv", header=FALSE) %>% sample_data(TRUE)

```

## C4.5 on Sonar dataset

### Testing on all data

Let's train a C4.5 classifier with all our dataset and then evaluate it using the same dataset.

```{r, results=FALSE}
model <- train(sonar_data$obs, sonar_data$class, method = "J48")
predictions <- predict(model, sonar_data$obs)
evaluate_performance(predictions, sonar_data$class) %>% data.frame %>% kable
```

As you can see from the results, the model was already trained with this data, so it's overfitted on this data. That's why all the metrics that high. It's a bad evaluation technique. That's why we are going to use k-fold cross validation for all our further analysis.

### Testing using k-fold validation

First, Let's train all the models we have with the sonar dataset.

```{r, results=FALSE, include=FALSE}
sonar_eval <- train_and_evaluate_all_models(sonar_data)
```

```{r}
sonar_eval$c45 %>% data.frame %>% kable
```

As you can see, using the k-fold cross validation made the evaluation much better.

## Other classifiers

### Random Forest

```{r}
sonar_eval$rf %>% data.frame %>% kable
```

### Support Vector Machine

```{r}
sonar_eval$svm %>% data.frame %>% kable
```


### Naive Bayes

```{r}
sonar_eval$nb %>% data.frame %>% kable
```


### Neural Network

```{r}
sonar_eval$nnet %>% data.frame %>% kable
```

### Bagging

```{r}
sonar_eval$bagging %>% data.frame %>% kable
```

### Boosting

```{r}
sonar_eval$boosting %>% data.frame %>% kable
```

Since the boosting is an ensambling technique, it outperformed most of the other models. The bagging and boosting are also outperformning the C4.5 tree that they are based on.

## Other datasets

Let's load other dataset and prepare them for the analysis

```{r, results=FALSE}
#hepatitis_eval <- train_and_evaluate_all_models(hepatitis_data)
spect_eval <- train_and_evaluate_all_models(spect_data)
pima_eval <- train_and_evaluate_all_models(pima_data)
```

### Dataset VS Model

Here we will be comparing each model against each dataset to have a better comparison.

#### Accuracy

```{r}
format_comparison("Accuracy") %>% sapply(as.data.frame) %>% t %>% kable
```

#### Precision

```{r}
format_comparison("Precision") %>% sapply(as.data.frame) %>% t %>% kable
```


#### Recall

```{r}
format_comparison("Recall") %>% sapply(as.data.frame) %>% t %>% kable
```


#### F1

```{r}
format_comparison("F1") %>% sapply(as.data.frame) %>% t %>% kable
```


## Wins

```{r}

who_win <- function(dataset,metric){
  tmp <- format_comparison(metric) %>% sapply(as.data.frame) %>% data.frame
  tmp
  
  sapply(models, function(model1){
    sets <- sapply(models, function(model2){
      unlist(tmp[model1,dataset]) > unlist(tmp[model2,dataset])
    })
    sum(sets)
  })
}
```

### Sonar

```{r}
who_win("sonar_eval", "Accuracy") %>% data.frame %>% t %>% kable(caption="Accuracy")
who_win("sonar_eval", "Precision") %>% data.frame %>% t %>% kable(caption="Precision")
who_win("sonar_eval", "Recall") %>% data.frame %>% t %>% kable(caption="Recall")
who_win("sonar_eval", "F1") %>% data.frame %>% t %>% kable(caption="F1")
```

In the sonar dataset, the boosting seems to outperform all other models in all the aspects (except for the precision), the second best model is the random forests.

### SPECT

```{r}
who_win("spect_eval", "Accuracy") %>% data.frame %>% t %>% kable(caption="Accuracy")
who_win("spect_eval", "Precision") %>% data.frame %>% t %>% kable(caption="Precision")
who_win("spect_eval", "Recall") %>% data.frame %>% t %>% kable(caption="Recall")
who_win("spect_eval", "F1") %>% data.frame %>% t %>% kable(caption="F1")
```

In the SPECT dataset, the bagging is wins all the metrics except the precision which the naive bays won.

### Pima

```{r}
who_win("pima_eval", "Accuracy") %>% data.frame %>% t %>% kable(caption="Accuracy")
who_win("pima_eval", "Precision") %>% data.frame %>% t %>% kable(caption="Precision")
who_win("pima_eval", "Recall") %>% data.frame %>% t %>% kable(caption="Recall")
who_win("pima_eval", "F1") %>% data.frame %>% t %>% kable(caption="F1")
```

In the pima dataset, there isn't a clear winner. Random Forests is competing with the SVM on the winner.


As these comparison show, there isn't the concept of a the best model for all datasets. It depends on the dataset used.