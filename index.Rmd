---
title: "Evaluation"
author: "Mohamed Bassem"
date: "April 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

```{r, results=FALSE}
library("dplyr")
library("caret")
library("knitr")
```


## Helper functions
```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           savePredictions = TRUE)
```
```{r}
evaluate_performance <- function(predicted, actual) {
  cmat <- confusionMatrix(predicted, actual)$table
  
  TP <- cmat[1,1]
  FN <- cmat[1,2]
  FP <- cmat[2,1]
  TN <- cmat[2,2]
  
  ret <- list(
    Accuracy = (TP+TN)/(TP+FN+FP+TN),
    Precision = (TP/(TP+FP)),
    Recall = (TP/(TP+FN))
  )
  
  ret$F1 <- (2*ret$Precision*ret$Recall)/(ret$Precision + ret$Recall)
  return(ret)
}

```
```{r}
sample_data <- function(data, class_column_last) {
  data <- data[, colSums(is.na(data)) == 0 ]
  #data <- data %>% sample_n(10)
  class_column <- 1
  if(class_column_last){
    class_column <- length(names(data))
  }
  
  if(is.integer(data[,class_column])){
    data[,class_column] <- as.factor(data[,class_column])
  }
  
  trainIdx <- createDataPartition(data[,class_column], p=.8, list=FALSE)
  train <- data[trainIdx, ]
  test <- data[-trainIdx, ]
  return(list(
    obs = data[,-class_column],
    class = data[,class_column]
  ))
}


```
```{r}
train_and_evaluate_model <- function(data, method){
  if(method == "nnet"){
    model <- train(data$obs, data$class, method = method, trControl=fitControl, trace=FALSE)
  }else{
    model <- train(data$obs, data$class, method = method, trControl=fitControl)
  }
  return(evaluate_performance(model$pred$pred, model$pred$obs))
}

```
```{r}
train_and_evaluate_all_models <- function(data){
  return(list(
    c45 = train_and_evaluate_model(data, "J48"),
    rf = train_and_evaluate_model(data, "rf"),
    svm = train_and_evaluate_model(data, "svmLinear"),
    nb = train_and_evaluate_model(data, "nb"),
    nnet = train_and_evaluate_model(data, "nnet"),
    bagging = train_and_evaluate_model(data, "treebag"),
    boosting = train_and_evaluate_model(data, "adaboost")
  ))
}
```
```{r}

datasets <- c("sonar_eval", "spect_eval", "pima_eval")
models <- c("c45", "rf", "svm", "nb", "nnet","bagging", "boosting")

format_comparison <- function(metric){
  ret <- list()
  
  for(dataset in datasets){
    ret[[dataset]] <- list()
    for(model in models){
      ret[[dataset]][[model]] <- get(dataset)[[model]][[metric]]
    }
  }
  return(ret)
}
```

## Loading Data

```{r}
sonar_data <- read.csv("./sonar/sonar.csv", header=FALSE) %>% sample_data(TRUE)
hepatitis_data <- read.csv("hepatitis/hepatitis.csv", header=FALSE) %>% dplyr::select(-V16, -V17) %>% sample_data(FALSE)
spect_data <- read.csv("SPECT/SPECT.train.csv", header = FALSE) %>% sample_data(FALSE)
pima_data <- read.csv("pima/pima-indians-diabetes.csv", header=FALSE) %>% sample_data(TRUE)

```

## Part 2

```{r, results=FALSE}
sonar_eval <- train_and_evaluate_all_models(sonar_data)
```

### Testing on all data

```{r, results=FALSE}
model <- train(sonar_data$obs, sonar_data$class, method = "J48")
predictions <- predict(model, sonar_data$obs)
evaluate_performance(predictions, sonar_data$class) %>% data.frame %>% kable
```

### Testing using k-fold validation

```{r}
sonar_eval$c45 %>% data.frame %>% kable
```

## Part 3

### Random Forest

```{r}
sonar_eval$rf %>% data.frame %>% kable
```

### Support Vector Machine

```{r}
sonar_eval$svm %>% data.frame %>% kable
```


### Naive Bayes

```{r}
sonar_eval$nb %>% data.frame %>% kable
```


### Neural Network

```{r}
sonar_eval$nnet %>% data.frame %>% kable
```

### Bagging

```{r}
sonar_eval$bagging %>% data.frame %>% kable
```

### Boosting

```{r}
sonar_eval$boosting %>% data.frame %>% kable
```

## Part 4

```{r, results=FALSE}
#hepatitis_eval <- train_and_evaluate_all_models(hepatitis_data)
spect_eval <- train_and_evaluate_all_models(spect_data)
pima_eval <- train_and_evaluate_all_models(pima_data)
```

### Accuracy

```{r}
format_comparison("Accuracy") %>% sapply(as.data.frame) %>% t %>% kable
```


### Precision

```{r}
format_comparison("Precision") %>% sapply(as.data.frame) %>% t %>% kable
```


### Recall

```{r}
format_comparison("Recall") %>% sapply(as.data.frame) %>% t %>% kable
```


### F1

```{r}
format_comparison("F1") %>% sapply(as.data.frame) %>% t %>% kable
```


## Wins

```{r}

who_win <- function(dataset,metric){
  tmp <- format_comparison(metric) %>% sapply(as.data.frame) %>% data.frame
  tmp
  
  sapply(models, function(model1){
    sets <- sapply(models, function(model2){
      unlist(tmp[model1,dataset]) > unlist(tmp[model2,dataset])
    })
    sum(sets)
  })
}
```

### Sonar

```{r}
who_win("sonar_eval", "Accuracy") %>% data.frame %>% t %>% kable
who_win("sonar_eval", "Precision") %>% data.frame %>% t %>% kable
who_win("sonar_eval", "Recall") %>% data.frame %>% t %>% kable
who_win("sonar_eval", "F1") %>% data.frame %>% t %>% kable
```


### SPECT

```{r}
who_win("spect_eval", "Accuracy") %>% data.frame %>% t %>% kable
who_win("spect_eval", "Precision") %>% data.frame %>% t %>% kable
who_win("spect_eval", "Recall") %>% data.frame %>% t %>% kable
who_win("spect_eval", "F1") %>% data.frame %>% t %>% kable
```


### Pima

```{r}
who_win("pima_eval", "Accuracy") %>% data.frame %>% t %>% kable
who_win("pima_eval", "Precision") %>% data.frame %>% t %>% kable
who_win("pima_eval", "Recall") %>% data.frame %>% t %>% kable
who_win("pima_eval", "F1") %>% data.frame %>% t %>% kable
```