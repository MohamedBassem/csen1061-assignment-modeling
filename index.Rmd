---
title: "Evaluation"
author: "Mohamed Bassem"
date: "April 15, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("dplyr")
library("caret")
library("knitr")
```


## Helper functions
```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           savePredictions = TRUE)
```
```{r}
evaluate_performance <- function(predicted, actual) {
  cmat <- confusionMatrix(predicted, actual)$table
  
  TP <- cmat[1,1]
  FN <- cmat[1,2]
  FP <- cmat[2,1]
  TN <- cmat[2,2]
  
  ret <- list(
    Accuracy = (TP+TN)/(TP+FN+FP+TN),
    Precision = (TP/(TP+FP)),
    Recall = (TP/(TP+FN))
  )
  
  ret$F1 <- (2*ret$Precision*ret$Recall)/(ret$Precision + ret$Recall)
  return(ret)
}

```
```{r}
sample_data <- function(data, class_column_last) {
  data <- data[, colSums(is.na(data)) == 0 ]
  #data <- data %>% sample_n(10)
  class_column <- 1
  if(class_column_last){
    class_column <- length(names(data))
  }
  
  if(is.integer(data[,class_column])){
    data[,class_column] <- as.factor(data[,class_column])
  }
  
  trainIdx <- createDataPartition(data[,class_column], p=.8, list=FALSE)
  train <- data[trainIdx, ]
  test <- data[-trainIdx, ]
  return(list(
    obs = data[,-class_column],
    class = data[,class_column]
  ))
}


```
```{r}
train_and_evaluate_model <- function(data, method){
  model <- train(data$obs, data$class, method = method, trControl=fitControl)
  return(evaluate_performance(model$pred$pred, model$pred$obs))
}

```
```{r}
train_and_evaluate_all_models <- function(data){
  return(list(
    c45 = train_and_evaluate_model(data, "J48"),
    rf = train_and_evaluate_model(data, "rf"),
    svm = train_and_evaluate_model(data, "svmLinear"),
    nb = train_and_evaluate_model(data, "nb"),
    nnet = train_and_evaluate_model(data, "nnet")
  ))
}
```
```{r}
format_comparison <- function(metric){
  datasets <- c("sonar_eval", "spect_eval", "pima_eval")
  models <- c("c45", "rf", "svm", "nb", "nnet")
  
  ret <- list()
  
  for(dataset in datasets){
    ret[[dataset]] <- list()
    for(model in models){
      ret[[dataset]][[model]] <- get(dataset)[[model]][[metric]]
    }
  }
  return(ret)
}
```

## Loading Data

```{r}
sonar_data <- read.csv("./sonar/sonar.csv", header=FALSE) %>% sample_data(TRUE)
hepatitis_data <- read.csv("hepatitis/hepatitis.csv", header=FALSE) %>% dplyr::select(-V16, -V17) %>% sample_data(FALSE)
spect_data <- read.csv("SPECT/SPECT.train.csv", header = FALSE) %>% sample_data(FALSE)
pima_data <- read.csv("pima/pima-indians-diabetes.csv", header=FALSE) %>% sample_data(TRUE)

```

## Part 2

```{r, results=FALSE}
sonar_eval <- train_and_evaluate_all_models(sonar_data)
```

### Testing on all data

```{r}
model <- train(sonar_data$obs, sonar_data$class, method = "J48")
predictions <- predict(model, sonar_data$obs)
evaluate_performance(predictions, sonar_data$class)
```

### Testing using k-fold validation

```{r}
sonar_eval$c45
```

## Part 3

### Random Forest

```{r}
sonar_eval$rf
```

### Support Vector Machine

```{r}
sonar_eval$svm
```


### Naive Bayes

```{r}
sonar_eval$nb
```


### Neural Network

```{r}
sonar_eval$nnet
```

### Bagging

### Boosting

## Part 4

```{r, results=FALSE}
#hepatitis_eval <- train_and_evaluate_all_models(hepatitis_data)
spect_eval <- train_and_evaluate_all_models(spect_data)
pima_eval <- train_and_evaluate_all_models(pima_data)
```

### Accuracy

```{r}
format_comparison("Accuracy") %>% sapply(as.data.frame) %>% t %>% kable
```


### Precision

```{r}
format_comparison("Precision") %>% sapply(as.data.frame) %>% t %>% kable
```


### Recall

```{r}
format_comparison("Recall") %>% sapply(as.data.frame) %>% t %>% kable
```


### F1

```{r}
format_comparison("F1") %>% sapply(as.data.frame) %>% t %>% kable
```

